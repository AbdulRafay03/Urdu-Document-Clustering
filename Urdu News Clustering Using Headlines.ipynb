{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory not found: C:\\Users\\Hp\\Desktop\\Urdu NEWS dataset\\bbc dataset\\miscellaneous\n",
      "Directory not found: C:\\Users\\Hp\\Desktop\\Urdu NEWS dataset\\voa dataset\\miscellaneous\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>HeadLines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'زیادہ پانی پینا بھی خطرناک ہو سکتا ہے'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'شاہ رخ سے رومانس نہ کرنے کا ملال نہیں'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'مومنہ ایک سیلفی پلیز'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'پرینکا مس ورلڈ بنیں تو میں سکول میں تھی'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'گیلیکسی نوٹ سیون کی پروڈکشن بند'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>پی ایس ایل اینٹی کرپشن یونٹ کی مزید تین کھلاڑی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>کرکٹ  پاکستان اور افغانستان کو قریب لا سکتا ہے...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>کوئٹہ گلیڈی ایٹرز کی مسلسل دوسری کامیابی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>یاسر شاہ کی عمدہ باؤلنگ، پاکستان دوسرے ٹیسٹ می...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>یوٹیوب کی مقبول ترین ویڈیوز</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2097 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dataset       Category  \\\n",
       "0     bbc dataset  entertainment   \n",
       "1     bbc dataset  entertainment   \n",
       "2     bbc dataset  entertainment   \n",
       "3     bbc dataset  entertainment   \n",
       "4     bbc dataset  entertainment   \n",
       "...           ...            ...   \n",
       "2092  voa dataset         sports   \n",
       "2093  voa dataset         sports   \n",
       "2094  voa dataset         sports   \n",
       "2095  voa dataset         sports   \n",
       "2096  voa dataset         sports   \n",
       "\n",
       "                                              HeadLines  \n",
       "0               'زیادہ پانی پینا بھی خطرناک ہو سکتا ہے'  \n",
       "1               'شاہ رخ سے رومانس نہ کرنے کا ملال نہیں'  \n",
       "2                                'مومنہ ایک سیلفی پلیز'  \n",
       "3           'پرینکا مس ورلڈ بنیں تو میں سکول میں تھی'    \n",
       "4                     'گیلیکسی نوٹ سیون کی پروڈکشن بند'  \n",
       "...                                                 ...  \n",
       "2092  پی ایس ایل اینٹی کرپشن یونٹ کی مزید تین کھلاڑی...  \n",
       "2093  کرکٹ  پاکستان اور افغانستان کو قریب لا سکتا ہے...  \n",
       "2094           کوئٹہ گلیڈی ایٹرز کی مسلسل دوسری کامیابی  \n",
       "2095  یاسر شاہ کی عمدہ باؤلنگ، پاکستان دوسرے ٹیسٹ می...  \n",
       "2096                        یوٹیوب کی مقبول ترین ویڈیوز  \n",
       "\n",
       "[2097 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Base path to the datasets\n",
    "path = r\"C:\\Users\\Hp\\Desktop\\Urdu NEWS dataset\"\n",
    "\n",
    "# List to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate over the dataset categories\n",
    "for dataset in ['bbc dataset', 'voa dataset']:\n",
    "    # Iterate over the news categories\n",
    "    for category in ['entertainment', 'miscellaneous', 'politics', 'sports']:\n",
    "        # Construct the full directory path\n",
    "        dir_path = os.path.join(path, dataset, category)\n",
    "        # List files and directories in the current directory\n",
    "        try:\n",
    "            dir_list = os.listdir(dir_path)\n",
    "            # Append each file/directory with its dataset and category\n",
    "            for item in dir_list:\n",
    "                # Remove the .doc extension\n",
    "                file_name, file_extension = os.path.splitext(item)\n",
    "                if file_extension in [\".doc\" , \".docx\" ]:\n",
    "                    item = file_name\n",
    "                data.append({\n",
    "                    'Dataset': dataset,\n",
    "                    'Category': category,\n",
    "                    'HeadLines': item\n",
    "                })\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Directory not found: {dir_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing {dir_path}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install lughaatNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Hp\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Hp\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from LughaatNLP import NER_Urdu\n",
    "from LughaatNLP import POS_urdu\n",
    "from LughaatNLP import LughaatNLP\n",
    "urdu_text_processing = LughaatNLP()\n",
    "pos_tagger = POS_urdu()\n",
    "ner_urdu = NER_Urdu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize(text): \n",
    "Performs all-in-one normalization on the Urdu text, including character normalization, diacritic removal, punctuation handling, digit conversion, and special character preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اپ کیسے ہیں ؟ میں ۲۳ سال کا ہوں ۔\n"
     ]
    }
   ],
   "source": [
    "text = \"آپ کیسے ہیں؟ میں 23 سال کا ہوں۔\"\n",
    "normalized_text = urdu_text_processing.normalize(text)\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>HeadLines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'زیادہ پانی پینا بھی خطرناک ہو سکتا ہے'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'شاہ رخ سے رومانس نہ کرنے کا ملال نہیں'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'مومنہ ایک سیلفی پلیز'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'پرینکا مس ورلڈ بنیں تو میں سکول میں تھی'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'گیلیکسی نوٹ سیون کی پروڈکشن بند'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>پی ایس ایل اینٹی کرپشن یونٹ کی مزید تین کھلاڑی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>کرکٹ پاکستان اور افغانستان کو قریب لا سکتا ہے ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>کوئٹہ گلیڈی ایٹرز کی مسلسل دوسری کامیابی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>یاسر شاہ کی عمدہ باؤلنگ، پاکستان دوسرے ٹیسٹ می...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>یوٹیوب کی مقبول ترین ویڈیوز</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2097 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dataset       Category  \\\n",
       "0     bbc dataset  entertainment   \n",
       "1     bbc dataset  entertainment   \n",
       "2     bbc dataset  entertainment   \n",
       "3     bbc dataset  entertainment   \n",
       "4     bbc dataset  entertainment   \n",
       "...           ...            ...   \n",
       "2092  voa dataset         sports   \n",
       "2093  voa dataset         sports   \n",
       "2094  voa dataset         sports   \n",
       "2095  voa dataset         sports   \n",
       "2096  voa dataset         sports   \n",
       "\n",
       "                                              HeadLines  \n",
       "0               'زیادہ پانی پینا بھی خطرناک ہو سکتا ہے'  \n",
       "1               'شاہ رخ سے رومانس نہ کرنے کا ملال نہیں'  \n",
       "2                                'مومنہ ایک سیلفی پلیز'  \n",
       "3             'پرینکا مس ورلڈ بنیں تو میں سکول میں تھی'  \n",
       "4                     'گیلیکسی نوٹ سیون کی پروڈکشن بند'  \n",
       "...                                                 ...  \n",
       "2092  پی ایس ایل اینٹی کرپشن یونٹ کی مزید تین کھلاڑی...  \n",
       "2093  کرکٹ پاکستان اور افغانستان کو قریب لا سکتا ہے ...  \n",
       "2094           کوئٹہ گلیڈی ایٹرز کی مسلسل دوسری کامیابی  \n",
       "2095  یاسر شاہ کی عمدہ باؤلنگ، پاکستان دوسرے ٹیسٹ می...  \n",
       "2096                        یوٹیوب کی مقبول ترین ویڈیوز  \n",
       "\n",
       "[2097 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['HeadLines'] = df['HeadLines'].apply(urdu_text_processing.normalize)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words Removing\n",
    "Stop words are common words in a language (such as “کہ”, “کیا”, “اور”, “لیکن”, “بھی”) that are often filtered out during text processing or analysis because they are considered irrelevant for tasks like searching or natural language understanding in Urdu language.\n",
    "\n",
    "This function removes stopwords from the Urdu text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "کتاب پڑھنا چاہتا ۔\n"
     ]
    }
   ],
   "source": [
    "text = \"میں اس کتاب کو پڑھنا چاہتا ہوں۔\"\n",
    "filtered_text = urdu_text_processing.remove_stopwords(text)\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Category</th>\n",
       "      <th>HeadLines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'زیادہ پانی پینا بھی خطرناک ہو سکتا ہے'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'شاہ رخ سے رومانس نہ کرنے کا ملال نہیں'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'مومنہ ایک سیلفی پلیز'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'پرینکا مس ورلڈ بنیں تو میں سکول میں تھی'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbc dataset</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>'گیلیکسی نوٹ سیون کی پروڈکشن بند'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>پی ایس ایل اینٹی کرپشن یونٹ کی مزید تین کھلاڑی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>کرکٹ پاکستان اور افغانستان کو قریب لا سکتا ہے ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>کوئٹہ گلیڈی ایٹرز کی مسلسل دوسری کامیابی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>یاسر شاہ کی عمدہ باؤلنگ، پاکستان دوسرے ٹیسٹ می...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>voa dataset</td>\n",
       "      <td>sports</td>\n",
       "      <td>یوٹیوب کی مقبول ترین ویڈیوز</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2097 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dataset       Category  \\\n",
       "0     bbc dataset  entertainment   \n",
       "1     bbc dataset  entertainment   \n",
       "2     bbc dataset  entertainment   \n",
       "3     bbc dataset  entertainment   \n",
       "4     bbc dataset  entertainment   \n",
       "...           ...            ...   \n",
       "2092  voa dataset         sports   \n",
       "2093  voa dataset         sports   \n",
       "2094  voa dataset         sports   \n",
       "2095  voa dataset         sports   \n",
       "2096  voa dataset         sports   \n",
       "\n",
       "                                              HeadLines  \n",
       "0               'زیادہ پانی پینا بھی خطرناک ہو سکتا ہے'  \n",
       "1               'شاہ رخ سے رومانس نہ کرنے کا ملال نہیں'  \n",
       "2                                'مومنہ ایک سیلفی پلیز'  \n",
       "3             'پرینکا مس ورلڈ بنیں تو میں سکول میں تھی'  \n",
       "4                     'گیلیکسی نوٹ سیون کی پروڈکشن بند'  \n",
       "...                                                 ...  \n",
       "2092  پی ایس ایل اینٹی کرپشن یونٹ کی مزید تین کھلاڑی...  \n",
       "2093  کرکٹ پاکستان اور افغانستان کو قریب لا سکتا ہے ...  \n",
       "2094           کوئٹہ گلیڈی ایٹرز کی مسلسل دوسری کامیابی  \n",
       "2095  یاسر شاہ کی عمدہ باؤلنگ، پاکستان دوسرے ٹیسٹ می...  \n",
       "2096                        یوٹیوب کی مقبول ترین ویڈیوز  \n",
       "\n",
       "[2097 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['HeadLines'] = df['HeadLines'].apply(urdu_text_processing.remove_stopwords)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'پی ایس ایل اینٹی کرپشن یونٹ کی مزید تین کھلاڑیوں سے پوچھ گچھ'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['HeadLines'].iloc[2092]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech\n",
    "The pos_tags_urdu function is used for part-of-speech tagging in Urdu text. It takes an Urdu sentence as input and returns a list of dictionaries where each word is paired with its assigned part-of-speech tag, such as nouns (NN), verbs (VB), adjectives (ADJ), etc.\n",
    "\n",
    "This Function will return dictionary words with their corresponding tags of Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 918ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'پی': 'B-ORGANIZATION',\n",
       " 'ایس': 'I-ORGANIZATION',\n",
       " 'ایل': 'L-ORGANIZATION',\n",
       " 'اینٹی': 'O',\n",
       " 'کرپشن': 'O',\n",
       " 'یونٹ': 'O',\n",
       " 'کی': 'O',\n",
       " 'مزید': 'O',\n",
       " 'تین': 'U-NUMBER',\n",
       " 'کھلاڑیوں': 'O',\n",
       " 'سے': 'O',\n",
       " 'پوچھ': 'O',\n",
       " 'گچھ': 'O'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'پی ایس ایل اینٹی کرپشن یونٹ کی مزید تین کھلاڑیوں سے پوچھ گچھ'\n",
    "word_tag_dict = ner_urdu.ner_tags_urdu(sentence)\n",
    "word_tag_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization for Urdu language: ['پی', 'ایس', 'ایل', 'اینٹی', 'کرپشن', 'یونٹ', 'کی', 'مزید', 'تین', 'کھلاڑیوں', 'سے', 'پوچھ', 'گچھ']\n",
      "This will return the most similar words in list with percentage [('پی', 100.0)]\n",
      "This will return the most similar words in list with percentage [('ایس', 100.0), ('ایسا', 75.0), ('ایسے', 75.0), ('ایسی', 75.0), ('ایکس', 75.0), ('ایسڈ', 75.0), ('ایپس', 75.0), ('ایلس', 75.0), ('ایسٹ', 75.0), ('انیس', 75.0), ('اسیس', 75.0), ('ایمس', 75.0), ('ایٹس', 75.0), ('دایس', 75.0), ('اکیس', 75.0), ('ایرس', 75.0), ('ایسز', 75.0)]\n",
      "This will return the most similar words in list with percentage [('ایل', 100.0), ('اپیل', 75.0), ('ایلن', 75.0), ('ایپل', 75.0), ('انیل', 75.0), ('ایلس', 75.0), ('ایبل', 75.0), ('ایلی', 75.0), ('ایگل', 75.0), ('ایفل', 75.0), ('ایول', 75.0), ('دایل', 75.0), ('ایمل', 75.0), ('ایڈل', 75.0), ('ایلک', 75.0), ('ایلٹ', 75.0)]\n",
      "This will return the most similar words in list with percentage [('اینٹی', 100.0), ('اینٹیں', 83.33333333333334), ('ایٹی', 80.0), ('ہینٹی', 80.0), ('وینٹی', 80.0), ('اینڈی', 80.0), ('سینٹی', 80.0), ('اينٹی', 80.0), ('اینی', 80.0), ('اینٹ', 80.0), ('اینجی', 80.0), ('انٹی', 80.0), ('ایوٹی', 80.0), ('ایسٹی', 80.0), ('ایمٹی', 80.0), ('فینٹی', 80.0), ('ایمنسٹی', 71.42857142857143), ('اینٹیفا', 71.42857142857143), ('اینٹیکو', 71.42857142857143), ('انفینٹی', 71.42857142857143)]\n",
      "This will return the most similar words in list with percentage [('کرپشن', 100.0), ('کرشن', 80.0), ('کیپشن', 80.0), ('کرپشٹ', 80.0)]\n",
      "This will return the most similar words in list with percentage [('یونٹ', 100.0), ('ایونٹ', 80.0), ('یونٹس', 80.0), ('یونٹی', 80.0), ('لیونٹ', 80.0), ('نیونٹ', 80.0), ('یونس', 75.0), ('اونٹ', 75.0), ('ہونٹ', 75.0), ('یونی', 75.0), ('یوٹ', 75.0), ('مونٹ', 75.0), ('ڈونٹ', 75.0), ('بونٹ', 75.0)]\n",
      "This will return the most similar words in list with percentage [('کی', 100.0)]\n",
      "This will return the most similar words in list with percentage [('مزید', 100.0), ('مفید', 75.0), ('مذید', 75.0), ('مزین', 75.0), ('مجید', 75.0), ('مرید', 75.0), ('مزيد', 75.0), ('معید', 75.0), ('زید', 75.0), ('یزید', 75.0)]\n",
      "This will return the most similar words in list with percentage [('تین', 100.0), ('ترین', 75.0), ('تعین', 75.0), ('متین', 75.0)]\n",
      "This will return the most similar words in list with percentage [('کھلاڑیوں', 100.0), ('کھلاڑی', 75.0), ('جھاڑیوں', 75.0)]\n",
      "This will return the most similar words in list with percentage [('سے', 100.0)]\n",
      "This will return the most similar words in list with percentage [('پوچھ', 100.0), ('پوچھا', 80.0), ('پونچھ', 80.0), ('پوچھے', 80.0), ('پوچھی', 80.0), ('سوچھ', 75.0)]\n",
      "This will return the most similar words in list with percentage [('گچھ', 100.0), ('گچھا', 75.0), ('گچھے', 75.0)]\n",
      "['پی', 'ایس', 'ایسا', 'ایسے', 'ایسی', 'ایکس', 'ایسڈ', 'ایپس', 'ایلس', 'ایسٹ', 'انیس', 'اسیس', 'ایمس', 'ایٹس', 'دایس', 'اکیس', 'ایرس', 'ایسز', 'ایل', 'اپیل', 'ایلن', 'ایپل', 'انیل', 'ایلس', 'ایبل', 'ایلی', 'ایگل', 'ایفل', 'ایول', 'دایل', 'ایمل', 'ایڈل', 'ایلک', 'ایلٹ', 'اینٹی', 'اینٹیں', 'ایٹی', 'ہینٹی', 'وینٹی', 'اینڈی', 'سینٹی', 'اينٹی', 'اینی', 'اینٹ', 'اینجی', 'انٹی', 'ایوٹی', 'ایسٹی', 'ایمٹی', 'فینٹی', 'ایمنسٹی', 'اینٹیفا', 'اینٹیکو', 'انفینٹی', 'کرپشن', 'کرشن', 'کیپشن', 'کرپشٹ', 'یونٹ', 'ایونٹ', 'یونٹس', 'یونٹی', 'لیونٹ', 'نیونٹ', 'یونس', 'اونٹ', 'ہونٹ', 'یونی', 'یوٹ', 'مونٹ', 'ڈونٹ', 'بونٹ', 'کی', 'مزید', 'مفید', 'مذید', 'مزین', 'مجید', 'مرید', 'مزيد', 'معید', 'زید', 'یزید', 'تین', 'ترین', 'تعین', 'متین', 'کھلاڑیوں', 'کھلاڑی', 'جھاڑیوں', 'سے', 'پوچھ', 'پوچھا', 'پونچھ', 'پوچھے', 'پوچھی', 'سوچھ', 'گچھ', 'گچھا', 'گچھے']\n"
     ]
    }
   ],
   "source": [
    "spell_checker = LughaatNLP()\n",
    "sentence = 'پی ایس ایل اینٹی کرپشن یونٹ کی مزید تین کھلاڑیوں سے پوچھ گچھ'\n",
    "tokens = urdu_text_processing.urdu_tokenize(sentence)\n",
    "print(\"Tokenization for Urdu language:\", tokens)  # Output: ['میں', 'پاکستان', 'سے', 'ہوں۔']\n",
    "l = []\n",
    "for i in tokens:\n",
    "    similar_words_with_percentage = spell_checker.get_similar_words_percentage(i, 70)\n",
    "    print(\"This will return the most similar words in list with percentage\", similar_words_with_percentage)\n",
    "    \n",
    "    for i in similar_words_with_percentage:\n",
    "        l.append(i[0])\n",
    "\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 668ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Word': 'پی', 'POS_Tag': 'PN'},\n",
       " {'Word': 'ایس', 'POS_Tag': 'PN'},\n",
       " {'Word': 'ایل', 'POS_Tag': 'PN'},\n",
       " {'Word': 'اینٹی', 'POS_Tag': 'PN'},\n",
       " {'Word': 'کرپشن', 'POS_Tag': 'PN'},\n",
       " {'Word': 'یونٹ', 'POS_Tag': 'NN'},\n",
       " {'Word': 'کی', 'POS_Tag': 'P'},\n",
       " {'Word': 'مزید', 'POS_Tag': 'ADJ'},\n",
       " {'Word': 'تین', 'POS_Tag': 'CA'},\n",
       " {'Word': 'کھلاڑیوں', 'POS_Tag': 'NN'},\n",
       " {'Word': 'سے', 'POS_Tag': 'SE'},\n",
       " {'Word': 'پوچھ', 'POS_Tag': 'VB'},\n",
       " {'Word': 'گچھ', 'POS_Tag': 'PN'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LughaatNLP import POS_urdu\n",
    "\n",
    "pos_tagger = POS_urdu()\n",
    "\n",
    "sentence = \"میرے والدین نے میری تعلیم اور تربیت میں بہت محنت کی تاکہ میں اپنی زندگی میں کامیاب ہو سکوں۔\"\n",
    "predicted_pos_tags = pos_tagger.pos_tags_urdu('پی ایس ایل اینٹی کرپشن یونٹ کی مزید تین کھلاڑیوں سے پوچھ گچھ')\n",
    "predicted_pos_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['HeadLines'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deep</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>state</th>\n",
       "      <th>words</th>\n",
       "      <th>أفغان</th>\n",
       "      <th>أفغانستان</th>\n",
       "      <th>أوباما</th>\n",
       "      <th>ئل</th>\n",
       "      <th>ئٹہ</th>\n",
       "      <th>...</th>\n",
       "      <th>۷۶</th>\n",
       "      <th>۷۷</th>\n",
       "      <th>۸۰</th>\n",
       "      <th>۸۰۰</th>\n",
       "      <th>۸۶</th>\n",
       "      <th>۸۷</th>\n",
       "      <th>۹۰</th>\n",
       "      <th>۹۰۰</th>\n",
       "      <th>۹۱</th>\n",
       "      <th>۹۲</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2097 rows × 4243 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      deep  negative  positive  state  words  أفغان  أفغانستان  أوباما   ئل  \\\n",
       "0      0.0       0.0       0.0    0.0    0.0    0.0        0.0     0.0  0.0   \n",
       "1      0.0       0.0       0.0    0.0    0.0    0.0        0.0     0.0  0.0   \n",
       "2      0.0       0.0       0.0    0.0    0.0    0.0        0.0     0.0  0.0   \n",
       "3      0.0       0.0       0.0    0.0    0.0    0.0        0.0     0.0  0.0   \n",
       "4      0.0       0.0       0.0    0.0    0.0    0.0        0.0     0.0  0.0   \n",
       "...    ...       ...       ...    ...    ...    ...        ...     ...  ...   \n",
       "2092   0.0       0.0       0.0    0.0    0.0    0.0        0.0     0.0  0.0   \n",
       "2093   0.0       0.0       0.0    0.0    0.0    0.0        0.0     0.0  0.0   \n",
       "2094   0.0       0.0       0.0    0.0    0.0    0.0        0.0     0.0  0.0   \n",
       "2095   0.0       0.0       0.0    0.0    0.0    0.0        0.0     0.0  0.0   \n",
       "2096   0.0       0.0       0.0    0.0    0.0    0.0        0.0     0.0  0.0   \n",
       "\n",
       "      ئٹہ  ...   ۷۶   ۷۷   ۸۰  ۸۰۰   ۸۶   ۸۷   ۹۰  ۹۰۰   ۹۱   ۹۲  \n",
       "0     0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "2092  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2093  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2094  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2095  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2096  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[2097 rows x 4243 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get feature names\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Display the DataFrame\n",
    "tfidf_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
